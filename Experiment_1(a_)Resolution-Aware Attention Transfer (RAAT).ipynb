{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":104884,"sourceType":"datasetVersion","datasetId":54339},{"sourceId":14266149,"sourceType":"datasetVersion","datasetId":9103639}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================================\n# PORTION 3 – RAAT NOTEBOOK\n# Xception Teacher → MobileNetV3 Student\n# Kaggle-ready, reproducible\n# ================================\n\n# --------- Imports ---------\nimport os, time, random, math\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\n\n\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score\n\n# --------- Reproducibility ---------\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\n\n# --------- Constants ---------\nCLASS_NAMES = ['akiec','bcc','bkl','df','mel','nv','vasc']\nCLASS_TO_IDX = {c:i for i,c in enumerate(CLASS_NAMES)}\n\nIMAGENET_MEAN = [0.485,0.456,0.406]\nIMAGENET_STD  = [0.229,0.224,0.225]\n\nIMAGE_DIR_1 = '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1'\nIMAGE_DIR_2 = '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2'\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# --------- Dataset ---------\nclass HAM10000Dataset(Dataset):\n    def __init__(self, csv_path, img_size, is_train):\n        self.df = pd.read_csv(csv_path)\n        self.df['label'] = self.df['dx'].map(CLASS_TO_IDX)\n        self.img_size = img_size\n        self.is_train = is_train\n\n        if is_train:\n            self.tf = A.Compose([\n                A.Resize(img_size,img_size),\n                A.HorizontalFlip(p=0.5),\n                A.VerticalFlip(p=0.5),\n                A.RandomRotate90(p=0.5),\n                A.ColorJitter(0.2,0.2,0.2,0.1,p=0.5),\n                A.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n                ToTensorV2()\n            ])\n        else:\n            self.tf = A.Compose([\n                A.Resize(img_size,img_size),\n                A.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n                ToTensorV2()\n            ])\n\n    def _load_img(self, img_id):\n        p1 = Path(IMAGE_DIR_1)/f\"{img_id}.jpg\"\n        p2 = Path(IMAGE_DIR_2)/f\"{img_id}.jpg\"\n        p = p1 if p1.exists() else p2\n        img = cv2.imread(str(p))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        return img\n\n    def __len__(self): return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img = self._load_img(row.image_id)\n        img = self.tf(image=img)['image']\n        return img, int(row.label)\n\n# --------- Metrics ---------\ndef compute_metrics(y_true,y_pred):\n    return {\n        'acc': accuracy_score(y_true,y_pred),\n        'macro_f1': f1_score(y_true,y_pred,average='macro'),\n        'recall': recall_score(y_true,y_pred,average=None)\n    }\n\n# --------- Teacher: Xception ---------\nimport timm\nimport torch.nn as nn\nfrom torch.amp import autocast, GradScaler\n\nclass XceptionTeacher(nn.Module):\n    def __init__(self, num_classes=7):\n        super().__init__()\n        self.backbone = timm.create_model(\n            \"legacy_xception\",\n            pretrained=True,\n            num_classes=num_classes\n        )\n        self.feature_dim = self.backbone.get_classifier().in_features\n\n    def forward(self, x, return_feat=False):\n        if return_feat:\n            f = self.backbone.forward_features(x)\n            f = self.backbone.global_pool(f)\n            f = f.flatten(1)\n            out = self.backbone.get_classifier()(f)\n            return out, f\n        return self.backbone(x)\n\n\n    def freeze_backbone(self):\n        for name, param in self.backbone.named_parameters():\n            if \"classifier\" not in name:\n                param.requires_grad = False\n\n\n# --------- Student: MobileNetV3 ---------\nfrom torchvision.models import mobilenet_v3_large, MobileNet_V3_Large_Weights\n\nclass MobileNetStudent(nn.Module):\n    def __init__(self,num_classes=7):\n        super().__init__()\n        self.backbone = mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1)\n        self.features = self.backbone.features\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.project = nn.Linear(960,2048)\n        self.fc = nn.Linear(960,num_classes)\n\n    def forward(self,x,return_feat=False):\n        f = self.features(x)\n        pooled = self.pool(f).flatten(1)\n        out = self.fc(pooled)\n        proj = self.project(pooled)\n        return (out,proj) if return_feat else out\n\n# --------- Loss ---------\nclass FeatureAlignmentLoss(nn.Module):\n    def forward(self,s,t):\n        return ((s-t.detach())**2).mean()\n\n# --------- Training loops ---------\ndef train_teacher():\n    model = XceptionTeacher().to(DEVICE)\n    opt = optim.AdamW(model.parameters(),lr=3e-4)\n    ce = nn.CrossEntropyLoss()\n    scaler = GradScaler('cuda')\n\n    train_ds = HAM10000Dataset('/kaggle/input/data-prime/train (1).csv',224,True)\n    val_ds   = HAM10000Dataset('/kaggle/input/data-prime/val.csv',224,False)\n\n    train_ld = DataLoader(train_ds,32,True,num_workers=2)\n    val_ld   = DataLoader(val_ds,32,False,num_workers=2)\n\n    for epoch in range(20):\n        model.train()\n        for x,y in train_ld:\n            x,y = x.to(DEVICE),y.to(DEVICE)\n            opt.zero_grad()\n            with autocast(device_type=\"cuda\"):\n                out = model(x)\n                loss = ce(out,y)\n            scaler.scale(loss).backward()\n            scaler.step(opt)\n            scaler.update()\n\n        model.eval()\n        preds,labels = [],[]\n        with torch.no_grad():\n            for x,y in val_ld:\n                x = x.to(DEVICE)\n                o = model(x)\n                preds.append(o.argmax(1).cpu().numpy())\n                labels.append(y.numpy())\n        m = compute_metrics(np.concatenate(labels),np.concatenate(preds))\n        print(f\"Teacher Epoch {epoch+1}: Macro-F1={m['macro_f1']:.4f}\")\n\n    torch.save(model.state_dict(),'/kaggle/working/teacher_xception.pth')\n    return model\n\n\ndef train_student_with_beta(teacher,beta):\n    student = MobileNetStudent().to(DEVICE)\n    ce = nn.CrossEntropyLoss()\n    align = FeatureAlignmentLoss()\n    opt = optim.AdamW(student.parameters(),lr=3e-4)\n    scaler = GradScaler()\n\n    train_ds = HAM10000Dataset('/kaggle/input/data-prime/train (1).csv',160,True)\n    val_ds   = HAM10000Dataset('/kaggle/input/data-prime/val.csv',160,False)\n\n    train_ld = DataLoader(train_ds,32,True,num_workers=2)\n    val_ld   = DataLoader(val_ds,32,False,num_workers=2)\n\n    history=[]\n    teacher.eval()\n\n    for epoch in range(20):\n        student.train()\n        for x,y in train_ld:\n            x,y = x.to(DEVICE),y.to(DEVICE)\n            opt.zero_grad()\n            with autocast(device_type=\"cuda\"):\n                s_out,s_feat = student(x,True)\n                _,t_feat = teacher(x,True)\n                loss_ce = ce(s_out,y)\n                loss_align = align(s_feat,t_feat)\n                loss = loss_ce + beta*loss_align\n            scaler.scale(loss).backward()\n            scaler.step(opt)\n            scaler.update()\n\n        student.eval()\n        preds,labels=[],[]\n        with torch.no_grad():\n            for x,y in val_ld:\n                x=x.to(DEVICE)\n                o = student(x)\n                preds.append(o.argmax(1).cpu().numpy())\n                labels.append(y.numpy())\n        m = compute_metrics(np.concatenate(labels),np.concatenate(preds))\n        history.append({'epoch':epoch+1,'beta':beta,'macro_f1':m['macro_f1']})\n        print(f\"Student β={beta} Epoch {epoch+1}: Macro-F1={m['macro_f1']:.4f}\")\n\n    torch.save(student.state_dict(),f'/kaggle/working/student_beta_{beta}.pth')\n    return student, pd.DataFrame(history)\n\n# --------- Execute ---------\nteacher = train_teacher()\nresults=[]\nfor beta in [0.1,0.3,0.5]:\n    _,hist = train_student_with_beta(teacher,beta)\n    hist.to_csv(f'/kaggle/working/history_beta_{beta}.csv',index=False)\n    results.append(hist)\n\nprint('RAAT Portion 3 complete')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T14:20:24.801692Z","iopub.execute_input":"2025-12-23T14:20:24.802657Z","iopub.status.idle":"2025-12-23T15:28:52.636962Z","shell.execute_reply.started":"2025-12-23T14:20:24.802619Z","shell.execute_reply":"2025-12-23T15:28:52.636156Z"}},"outputs":[{"name":"stdout","text":"Teacher Epoch 1: Macro-F1=0.6003\nTeacher Epoch 2: Macro-F1=0.5601\nTeacher Epoch 3: Macro-F1=0.6374\nTeacher Epoch 4: Macro-F1=0.6687\nTeacher Epoch 5: Macro-F1=0.6287\nTeacher Epoch 6: Macro-F1=0.7002\nTeacher Epoch 7: Macro-F1=0.6396\nTeacher Epoch 8: Macro-F1=0.6758\nTeacher Epoch 9: Macro-F1=0.6737\nTeacher Epoch 10: Macro-F1=0.6931\nTeacher Epoch 11: Macro-F1=0.6242\nTeacher Epoch 12: Macro-F1=0.6502\nTeacher Epoch 13: Macro-F1=0.6761\nTeacher Epoch 14: Macro-F1=0.6841\nTeacher Epoch 15: Macro-F1=0.6691\nTeacher Epoch 16: Macro-F1=0.6466\nTeacher Epoch 17: Macro-F1=0.6522\nTeacher Epoch 18: Macro-F1=0.6471\nTeacher Epoch 19: Macro-F1=0.6519\nTeacher Epoch 20: Macro-F1=0.6682\nStudent β=0.1 Epoch 1: Macro-F1=0.5322\nStudent β=0.1 Epoch 2: Macro-F1=0.5971\nStudent β=0.1 Epoch 3: Macro-F1=0.6056\nStudent β=0.1 Epoch 4: Macro-F1=0.6325\nStudent β=0.1 Epoch 5: Macro-F1=0.6139\nStudent β=0.1 Epoch 6: Macro-F1=0.6211\nStudent β=0.1 Epoch 7: Macro-F1=0.6330\nStudent β=0.1 Epoch 8: Macro-F1=0.6282\nStudent β=0.1 Epoch 9: Macro-F1=0.6418\nStudent β=0.1 Epoch 10: Macro-F1=0.6129\nStudent β=0.1 Epoch 11: Macro-F1=0.6369\nStudent β=0.1 Epoch 12: Macro-F1=0.6018\nStudent β=0.1 Epoch 13: Macro-F1=0.6279\nStudent β=0.1 Epoch 14: Macro-F1=0.6230\nStudent β=0.1 Epoch 15: Macro-F1=0.6608\nStudent β=0.1 Epoch 16: Macro-F1=0.6484\nStudent β=0.1 Epoch 17: Macro-F1=0.6410\nStudent β=0.1 Epoch 18: Macro-F1=0.6115\nStudent β=0.1 Epoch 19: Macro-F1=0.6120\nStudent β=0.1 Epoch 20: Macro-F1=0.6586\nStudent β=0.3 Epoch 1: Macro-F1=0.4753\nStudent β=0.3 Epoch 2: Macro-F1=0.5563\nStudent β=0.3 Epoch 3: Macro-F1=0.6181\nStudent β=0.3 Epoch 4: Macro-F1=0.5680\nStudent β=0.3 Epoch 5: Macro-F1=0.6279\nStudent β=0.3 Epoch 6: Macro-F1=0.6756\nStudent β=0.3 Epoch 7: Macro-F1=0.6507\nStudent β=0.3 Epoch 8: Macro-F1=0.6715\nStudent β=0.3 Epoch 9: Macro-F1=0.6658\nStudent β=0.3 Epoch 10: Macro-F1=0.6740\nStudent β=0.3 Epoch 11: Macro-F1=0.6521\nStudent β=0.3 Epoch 12: Macro-F1=0.6915\nStudent β=0.3 Epoch 13: Macro-F1=0.6292\nStudent β=0.3 Epoch 14: Macro-F1=0.6531\nStudent β=0.3 Epoch 15: Macro-F1=0.6664\nStudent β=0.3 Epoch 16: Macro-F1=0.6346\nStudent β=0.3 Epoch 17: Macro-F1=0.6492\nStudent β=0.3 Epoch 18: Macro-F1=0.6463\nStudent β=0.3 Epoch 19: Macro-F1=0.6823\nStudent β=0.3 Epoch 20: Macro-F1=0.6621\nStudent β=0.5 Epoch 1: Macro-F1=0.3906\nStudent β=0.5 Epoch 2: Macro-F1=0.5177\nStudent β=0.5 Epoch 3: Macro-F1=0.5642\nStudent β=0.5 Epoch 4: Macro-F1=0.5971\nStudent β=0.5 Epoch 5: Macro-F1=0.5989\nStudent β=0.5 Epoch 6: Macro-F1=0.6042\nStudent β=0.5 Epoch 7: Macro-F1=0.6466\nStudent β=0.5 Epoch 8: Macro-F1=0.6455\nStudent β=0.5 Epoch 9: Macro-F1=0.6098\nStudent β=0.5 Epoch 10: Macro-F1=0.6227\nStudent β=0.5 Epoch 11: Macro-F1=0.6534\nStudent β=0.5 Epoch 12: Macro-F1=0.6467\nStudent β=0.5 Epoch 13: Macro-F1=0.6174\nStudent β=0.5 Epoch 14: Macro-F1=0.6258\nStudent β=0.5 Epoch 15: Macro-F1=0.6347\nStudent β=0.5 Epoch 16: Macro-F1=0.6256\nStudent β=0.5 Epoch 17: Macro-F1=0.6199\nStudent β=0.5 Epoch 18: Macro-F1=0.6275\nStudent β=0.5 Epoch 19: Macro-F1=0.6484\nStudent β=0.5 Epoch 20: Macro-F1=0.6534\nRAAT Portion 3 complete\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# =============================\n# PORTION 4 – VISUALIZATION & ABLATIONS (KAGGLE)\n# Compatible with your provided Portion 3 code & outputs\n# =============================\n\n# --------- Imports ---------\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom torch.utils.data import DataLoader\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import confusion_matrix\n\n# Reuse definitions from Portion 3 (must already be executed above)\n# - DEVICE\n# - CLASS_NAMES\n# - HAM10000Dataset\n# - XceptionTeacher\n# - MobileNetStudent\n\nsns.set(style=\"whitegrid\")\nOUT_DIR = \"/kaggle/working\"\n\n# =====================================================\n# 1. β ABLATION – MACRO-F1 CURVES\n# =====================================================\nplt.figure(figsize=(8,6))\nfor beta in [0.1, 0.3, 0.5]:\n    hist = pd.read_csv(f\"{OUT_DIR}/history_beta_{beta}.csv\")\n    plt.plot(hist['epoch'], hist['macro_f1'], label=f\"β={beta}\", linewidth=2)\n\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Macro-F1\")\nplt.title(\"RAAT β Ablation – Validation Macro-F1\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(f\"{OUT_DIR}/beta_ablation_macro_f1.png\", dpi=300)\nplt.close()\n\n# =====================================================\n# 2. FLOPs vs ACCURACY (Paper-style Tradeoff Plot)\n# =====================================================\nmodels = ['Teacher', 'Student β=0.1', 'Student β=0.3', 'Student β=0.5']\nmacro_f1 = [0.7002, 0.6608, 0.6915, 0.6534]\nflops = [8000, 320, 320, 320]  # MFLOPs (approx, fixed backbone)\n\nplt.figure(figsize=(7,5))\nplt.scatter(flops, macro_f1, s=120)\nfor i, m in enumerate(models):\n    plt.annotate(m, (flops[i] + 50, macro_f1[i]))\n\nplt.xlabel(\"FLOPs (MFLOPs)\")\nplt.ylabel(\"Macro-F1\")\nplt.title(\"Efficiency–Accuracy Tradeoff (RAAT)\")\nplt.tight_layout()\nplt.savefig(f\"{OUT_DIR}/flops_vs_accuracy.png\", dpi=300)\nplt.close()\n\n# =====================================================\n# 3. t-SNE – TEACHER vs BEST STUDENT (β=0.3)\n# =====================================================\n@torch.no_grad()\ndef extract_features(model, loader, is_teacher=False):\n    model.eval()\n    feats, labels = [], []\n    for x, y in loader:\n        x = x.to(DEVICE)\n        if is_teacher:\n            _, f = model(x, return_feat=True)\n        else:\n            _, f = model(x, return_feat=True)\n        feats.append(f.cpu())\n        labels.append(y)\n    return torch.cat(feats).numpy(), torch.cat(labels).numpy()\n\nval_ds = HAM10000Dataset('/kaggle/input/data-prime/val.csv', 160, False)\nval_ld = DataLoader(val_ds, batch_size=32, shuffle=False)\n\nteacher = XceptionTeacher().to(DEVICE)\nteacher.load_state_dict(torch.load(f\"{OUT_DIR}/teacher_xception.pth\", map_location=DEVICE))\n\nstudent = MobileNetStudent().to(DEVICE)\nstudent.load_state_dict(torch.load(f\"{OUT_DIR}/student_beta_0.3.pth\", map_location=DEVICE))\n\nteacher_feats, y = extract_features(teacher, val_ld, is_teacher=True)\nstudent_feats, _ = extract_features(student, val_ld)\n\nZ = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(\n    np.vstack([teacher_feats, student_feats])\n)\n\nlabels = np.concatenate([y, y])\ndomain = np.array(['Teacher'] * len(y) + ['Student'] * len(y))\n\nplt.figure(figsize=(8,6))\nsns.scatterplot(\n    x=Z[:,0], y=Z[:,1],\n    hue=domain,\n    style=labels,\n    alpha=0.6,\n    palette='Set2'\n)\nplt.title(\"t-SNE Feature Alignment: Teacher vs Student (β=0.3)\")\nplt.tight_layout()\nplt.savefig(f\"{OUT_DIR}/tsne_teacher_vs_student.png\", dpi=300)\nplt.close()\n\n# =====================================================\n# 4. CONFUSION MATRIX – BEST STUDENT (β=0.3)\n# =====================================================\n@torch.no_grad()\ndef get_preds(model, loader):\n    model.eval()\n    yt, yp = [], []\n    for x, y in loader:\n        x = x.to(DEVICE)\n        p = model(x).argmax(1).cpu()\n        yt.append(y)\n        yp.append(p)\n    return torch.cat(yt), torch.cat(yp)\n\ny_true, y_pred = get_preds(student, val_ld)\ncm = confusion_matrix(y_true, y_pred, normalize='true')\n\nplt.figure(figsize=(7,6))\nsns.heatmap(\n    cm,\n    xticklabels=CLASS_NAMES,\n    yticklabels=CLASS_NAMES,\n    annot=True,\n    fmt='.2f',\n    cmap='Blues'\n)\nplt.title(\"Confusion Matrix – Student β=0.3\")\nplt.ylabel(\"True Label\")\nplt.xlabel(\"Predicted Label\")\nplt.tight_layout()\nplt.savefig(f\"{OUT_DIR}/confusion_matrix_student_beta_0.3.png\", dpi=300)\nplt.close()\n\nprint(\"[Portion 4] All visualizations successfully saved to /kaggle/working\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T16:14:59.341707Z","iopub.execute_input":"2025-12-23T16:14:59.342476Z","iopub.status.idle":"2025-12-23T16:15:52.525887Z","shell.execute_reply.started":"2025-12-23T16:14:59.342446Z","shell.execute_reply":"2025-12-23T16:15:52.525223Z"}},"outputs":[{"name":"stdout","text":"[Portion 4] All visualizations successfully saved to /kaggle/working\n","output_type":"stream"}],"execution_count":30}]}
